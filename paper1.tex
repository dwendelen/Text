\input{commands}



\documentclass[11pt]{IEEEtran}

\usepackage{amsfonts, amsmath}
\usepackage[english]{babel}

\title{Parallelisation of calculations with matrices using OpenCL}
\author{Daan Wendelen}

\begin{document}
\maketitle

\section{Tensors}
We can represent a tensor as a $N$-dimensional array. A $N$-dimensional tensor is called a $N$-th order tensor. A $N$-th order tensor has $N$ modes. A tensor in defined as:
\[
	\tensor{T} \in \R^{I_1 \times \dotsb \times I_N}
\]
We also define the collection of all valid indices of a tensor as:
\[
    \I = \{\ii{} \in \N_0^N | \forall n \in [1, N]: i_n \in [1, I_n]\}
\]

Tensors can be decomposed. One of those decompositions is the Canonical Polyadic Decomposition (CPD). The elements of \TT{} can be calculates in the following manner: 
\[
    t_{\ii{}} \approx c_{\ii{}} = \sum_{r=1}^{R} \prod_{n=1}^{N} u^{(n)}_{i_{n}r}
\]
\centering{with $\ii{} \in \I$, $R \in \N_0$ and $\mUU{n} \in \R^{I_n \times R} $}

The CPD \CC{} is an approximation of \TT{}. We therefor define the residu tensor as:
\[
    \F = \C - \T
\]
We define the goal function $f(\T, \mUUU)$ as the frobenius norm of \FF{}.
\[
    f(\T, \mUUU) = \sum_{\ii{} \in \I} \left( \left( \sum_{r=1}^{R} \prod_{n=1}^{N} u^{(n)}_{i_{n}r} \right) - t_{\ii{}} \right)^2
\]
The goal of many algorithms is to find the factormatrices \UUU{} for which $f$ is the smallest. One group of such algorithms are the gradient based optimization algorithms. They have in common that they use the gradient of $f$ to find the best approximation of \TT. The gradient can be calculated with the following formula.
\[
    g^{(n)}_{ir} :=\sum_{\substack{\iii{} \in \I\\ i'_n = i}} f_{\iii{}} \prod_{\substack{n'=1\\n'\neq n}}^{N} u^{(n')}_{i'_{n'}r}
\]
We call \GGG{} the gradientmatrices.

\section{OpenCL}
OpenCL is a framework for parallel computation. We can use OpenCL on both CPU's and on GPU's. We will focus on the GPU.

A GPU consist of a number of compute-units. Every compute-unit can run 64 threads in parallel. These 64 threads always execute
the same instruction at the same time. This means that one thread can stall the whole compute unit. A set of 64 threads is called a wavefront. And a compute unit can switch between wavefronts. A wavefront always runs on the same compute unit.

The GPU also has memory. The memory is served by several channels. When multiple compute-units try to access the same channel, at the same time, a channel conflict occurs. When this happens, one of the compute units stalls. A compute unit can access multiple channels at the same time. When we request data from the memory it takes 300 to 600 cycles to arrive. This is called the latency of the memory. It is important to have enough threads active to occupy the compute unit while another thread is waiting for the data to arrive.

\section{Kernels for calculating \FF{}}
We focussed on thirth order tensor. In our research we focussed on thirth order tensors.

\subsection{Is it suitable for parallelisation?}
Before we develop a kernel, we first do some research to validate that the algorithm is suitable to run in parallel on the graphics card. We validate that the algorithm is able to use all the computational power of the GPU. Some algorithms are not suitable because they are bound by the bandwidth. For other algorithms it is simply not possible to parallelise them.

We call the ratio between the number of flop's and the amount of memory transactions, the computation ratio. A high ratio is preferred because it means we can utilise more computational power. (if it is available) A low ratio indicates that the memory will be the bottleneck.

Our research indicated that $f$ is suitable for parallelisation when $I > 40$ and $R > 15$ for calculations in double precision. In single precision $I$ must be greater than 40 and $R$ must be greater than 31.

\subsection{The kernels}
 We developed several kernels. The first one the float8x8x8. With the float8x8x8, every wavefront is assigned a $8 \times 8 \times 8$ block of the tensor. This is about just enough to balance the speed of the compute unit, with the speed of the memorf. A variant on this kernel is the float16x16x16. With this kernel every wavefront is assigned a $16 \times 16 \times 16$ block of the tensor. As the name suggested. The float16x16x16 has a much better computation ratio.
 
 The problem with these kernels is that all wavefronts access multiple channels of the memory. This leads to channel conflicts and channel conflicts lead to a drop in performance. To tackle this issue, we also developed a few kernel which suffer less from this phenomenon. The strategy is to relocate the elements of \TT to locations that cause fewer channel conflicts.   The new kernels are called float16x16x16R (Remapped), float16x16x16I (Isolated) and float8x8x8R.

 
\section{Kernels for calculating \GGG{}}
\subsection{Modifying double16x16x16}
We need to use \FF{} to calculate the gradient. We can get \FF{} as a side effect of calculating $f$. We therefor need to modify for example double16x16x16 to write \FF{} to the buffer. The modified version is called double16x16x16FG. Note we use doubles. All algorithms can be modified to use double's instead of float's. This modification halfs the computation ratio. double16x16x16FG is suitable for parallelisation when $I > 40$ and $R > 30$ for calculations in double precision. In single precision $I$ must be greater than 40 and $R$ must be greater than 62.

\subsection{Is it suitable for parallelisation?}
We also need to verified that calculation of the gradient is suitable for parallelisation. The computation ratio of for computing the gradient is about the same as the ratio for computing $F$. We conclude that the gradient is suitable for parallelisation when $I > 40$ and $R > 15$ for calculations in double precision. In single precision $I$ must be greater than 40 and $R$ must be greater than 31.

\subsection{double16x16G}
The kernel that calculates the gradient is called double16x16G. We gave it that name because a wavefront is assigned to a $16 \times 16$ block of the gradientmatrix.

\section{Results}

\section{Conclusion}

\end{document} 

