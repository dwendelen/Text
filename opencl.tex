\chapter{OpenCL}
\label{h:opencl} 
\todo{PE ook gekend als SC}
OpenCL is een framework voor het uitvoeren van parallelle berekeningen. Deze berekeningen kunnen uitgevoerd worden op \'e\'en of meerdere gewone processoren (CPU) of op \'e\'en of meerdere grafische kaarten. Een CPU of een GPU noemen we een apparaat (device). We zullen het OpenCL-jargon niet vertalen naar het Nederlands om verwarring te vermijden.

\section{Het model}
In dit hoofdstuk beschrijven we de voornaamste OpenCL-concepten. Deze zijn onafhankelijk van het apparaat en zelfs van het soort apparaat.
\subsection{Verdeling van het werk}
\subsubsection{Work-items}
Een reeks van een instructies die parallel uitgevoerd kan worden vormen een kernel. De eenheid van werk dat door een kernel verwerkt wordt noemen we een work-item. Het apparaat voert de kernel, in parallel, uit op de work-items. Een work-item komt dus overeen met \'e\'en thread. Het aantal work-items is meestal afhankelijk van de invoer van het algoritme.

Neem als voorbeeld het optellen van twee vectoren. \vect{c} = \vect{a} + \vect{b}. We kunnen dit op een seri\"ele manier berekenen als volgt:
\lstset{language=C, basicstyle=\footnotesize\ttfamily, keepspaces=true}
\begin{lstlisting}
void sum(const double a[], const double b[],
        double c[], int N)
{
    for(int index = 0; index < N; index++)
    {
        c[index] = a[index] + b[index];
    }
}
\end{lstlisting}
We kunnen dit herschrijven naar:
\begin{lstlisting}
void sum(const double a[], const double b[], double c[], int N)
{
    for(int index = 0; index < N; index++)
    {
        inner(a, b, c, index);
    }
}
void inner(const double a[], const double b[], double c[],
    const int index)
{
    c[index] = a[index] + b[index];
}
\end{lstlisting}
In welke volgorde we de index kiezen maakt eigenlijk niets uit. We kunnen deze berekening dus makkelijk in parallel berekenen.
Een OpenCL-kernel uit een programma die dezelfde berekening doet kan er als volgt uitzien:
\begin{lstlisting}
__kernel void innerKernel(__global const double a[],
    __global const double b[], __global double c[])
{
	int i = get_global_id(0);
	c[i] = a[i] + b[i];
}
\end{lstlisting}
Het eerste dat opvalt is dat er een paar keywords zoals \naam{\_\_kernel} en \naam{\_\_global} zijn bij gekomen. \naam{\_\_kernel} geef aan dat de functie een kernel is die gelanceerd kan worden. \naam{\_\_global} geeft aan dat het om globaal geheugen gaat. (Zie \ref{h:opencl:geheugen})

Wat echter veel belangrijker is, is dat de parameter \naam{N} verdwenen is. Waar we vroeger \naam{inner} \naam{N} keren na elkaar uitvoerden, gaan we nu \naam{innerKernel} \naam{N} keer in parallel uitvoeren. Er zijn dus \naam{N} work-items. 

In dit voorbeeld komt \'e\'en element uit de vector overeen met \'e\'en work-item, maar dit moet niet altijd zo zijn. De kernel gebruikt \naam{get\_global\_id(0)} om te achterhalen welk work-item verwerkt wordt.

\subsubsection{NDRange}
In het bovenstaande voorbeeld associ\"eren we met met work-item een nummer. Het eerste work-item krijgt nummer nul, de tweede nummer \'e\'en enzovoorts. Het is ook mogelijk om met elk work-item een tweedimensionale co\"ordinaat te associ\"eren. Dit kan bijvoorbeeld nuttig zijn als we matrices willen optellen. We kunnen dan \'e\'en work-item per element van de matrix nemen. De eerste component van de co\"ordinaat komt dan overeen met de rij, en de tweede met de kolom. OpenCL laat zelfs driedimensionale co\"ordinaten toe, maar geen vierdimensionale co\"ordinaten.

We zeggen dat het volledige werk overeen komt met de NDRange en dat de NDRange verdeeld wordt in work-items. Wanneer we een kernel lanceren, geven we mee hoe groot de NDRange is. Of met andere woorden, hoeveel work-items er zijn. NDRange staat voor N dimensionele Range. Wanneer we een gewoon getal associ\"eren met een work-item spreken we over een 1DRange. Wanneer we tweedimensionale coordinaten gebruiken spreken we over een 2DRange en bij drie dimensies over een 3DRange. 

\subsubsection{Work-groups}
OpenCL verplicht ons om de NDRange op te delen een groepen. Zo \'e\'en groep noemen we een work-group. Waarom we deze groepen moeten maken zullen we later zien. Elke work-group is even groot en de grootte moet een gehele deler zijn van de grootte van de volledige NDRange. Dit betekent dat het aantal work-items een geheel veelvoud moet zijn van de grootte van een work-group. AMD beperkt de grootte van de work-groups tot maximaal 256 work-items.

\todo{meerdere dimensies}


\subsection{Geheugen}
\label{h:opencl:geheugen}
OpenCL kent drie soorten geheugen:
\begin{itemize}
    \item Privaat geheugen: enkel toegankelijk binnen work-item.
    \item Lokaal geheugen: gedeeld met alle work-items in binnen een work-group, niet toegankelijk buiten de work-group.
    \item Globaal geheugen: toegankelijk voor alle work-items.
\end{itemize}

Gegevens in het RAM-geheugen kunnen enkel gekopi\"eerd worden naar het globaal geheugen. Een work-item kan dan de gegevens lezen uit het globaal geheugen en opslaan in het privaat geheugen. Work-items kunnen ook gegevens schrijven naar het lokaal geheugen. Deze gegevens kunnen dan door andere work-items binnen dezelfde work-group gelezen worden. \todo{tekening: GG, WI, WG, LG, PG}

\subsection{Synchronisatie}
Niet alle work-items lopen synchroon. Dit kan voor problemen zorgen wanneer een work-item gegevens moet lezen die door een ander work-item geschreven zijn. Daarom is het mogelijk om synchronizatiepunten toe te voegen. Dit kan op twee niveau's. We kunnen alle work-items in een work-group synchroniseren en alle work-items in de NDRange. Wanneer een work-item een synchronizatiepunt tegen komt zal het daar stoppen met uitvoeren tot alle andere work-items stil staan aan het synchronizatiepunt. Dit heeft meestal een negatief effect op de performantie en moet zo veel mogelijk vermeden worden.

\section{<stuff over cpu en gpu>}

\section{Architectuur AMD Radeon HD 6970}
\subsection{Rekeneenheden}

Wanneer we een kernel lanceren gaat de GPU de work-groups verdelen over de compute-units. Elke compute-unit (CU) heeft \'e\'en command-processor die verantwoordelijk is voor het selecteren en ophalen van de instructies. Omdat elke CU een eigen command-processor heeft, werken CU's volledig zelfstandig en los van alle andere CU's. \todo{verschil verschillende producten binnen familie is aantal CU's}

De compute-unit bestaat, naast de command-processor, ook uit 16 processing-elementen (PE). Alle processing-elementen binnen een compute-unit worden aangestuurd door de command-processor in de compute-unit. Hierdoor voeren alle processing-elementen altijd dezelfde instructie uit.

Een processing-element is ook gekend onder de naam stream core (SC). Een ALU wordt in de documentatie soms ook processing element genoemd. In deze tekst zullen we altijd de termen gebruiken zoals ze in bovenstaande paragraaf beschreven zijn.

Zo \'e\'en instructie is 128 bits lang en bestaan eigenlijk uit vier 32-bit instructies. Een 128-bit instructie noemen we een very long instruction word (VILW). Een processing-elementen bestaat uit vier ALU's die elk een 32-bit instructie uitvoeren. Sommige instructies zijn te zwaar om met \'e\'en ALU uit te voeren. Daarom kunnen twee of vier ALU'a samengenomen worden om de zware instructie uit te voeren.\todo{<TODO>}

Misschien een beetje over andere families schrijven om context meer context. Is wel info die niet bijdraagt tot het begrijpen van de ontwerpbeslissingen vd kernels
\todo{</TODO>}

\todo{Wavefront en relatie met work-items (misschien mapping hardware <-> workitems een apart hfdstk}

\todo{Optimalisaties}

\todo{tekening uit slide/handleiding}
\todo{Ultra-Threaded Dispatch Processor}
\todo{latency van 8 cycli}

\subsection{Geheugen}

\subsubsection{Lokaal geheugen}
Elke compute-unit heeft 32KiB lokaal geheugen. Dit geheugen wordt gedeeld door alle work-groups \todo{enkel op dat moment gescheduled zijn} op de compute-unit. Het is voor een work-group echter niet mogelijk om te communiceren met een andere work-group via dit lokaal geheugen. Het lokaal geheugen bestaat uit 32 banken die elk vier bytes breed zijn. Dit betekent dat de eerste 4 bytes in de eerste bank gaan, de volgende vier in de tweede enzovoorts. De 129ste byte gaat terug in de eerste bank. ($4 \times 32 = 128$)

Elk processing-element kan elke cyclus twee vierbyte aanvragen doen. Deze twee adressen moeten niet aaneensluitend zijn. Elke cyclus kan elke bank \'e\'en aanvraag behandelen. Indien de 32 $(2 \times 16)$ vierbyte aanvragen allemaal naar verschillende bank gaan, kan men in theorie snelheden tot 2,7 TB/s over de hele GPU halen. Wanneer verschillende aanvragen gelijktijdig naar dezelfde bank gaan, worden de aanvragen geserialiseerd. We spreken dan van een bankconflict. De compute-unit blijft dan dezelfde wavefront uitvoeren. In elke cyclus zal dan \'e\'en aanvraag behandeld worden. Dit blijft duren tot elke aanvraag behandeld is.\todo{eventueel gevolg hele wavefront neerschrijven}
\todo{index flexibiliteit}

\subsubsection{Registers}
Elke compute-unit heeft 16Ki registers van elk vier keer vier bytes. Dit geeft in totaal 256KiB en is dus acht keer zo groot als het lokaal geheugen. De pool van registers worden gedeeld door alle work-items van alle work-groups op de compute-unit. \todo{enkel op dat moment gescheduled zijn} \todo{kan niet gedeeld worden} Elke cyclus kan elk processing-element twaalf vierbytes lezen uit het registergeheugen. Dit is juist genoeg om elke ALU een multiply-add in enkele precisie te laten uitvoeren in \'e\'en cyclus.\todo{verdeling deftig uitleggen}

\subsubsection{Globaal geheugen}
De GPU heeft 2GiB geheugen waarvan we 1GiB als globaal gedeeld geheugen kunnen gebruiken. Dit geheugen wordt gedeeld door alle work-items over het hele apparaat. Wanneer een wavefront een geheugen aanvraag doet, zal de compute-unit dit bundelen en doorsturen naar \'e\'en van de acht geheugencontrollers. Elke controller is op zijn beurt verbonden met meerdere banken. We zeggen de geheugencontroller de gegevens verwerken die naar \'e\'en kanaal gaan. Er is \'e\'en geheugencontroller per kanaal.

De controller verwerkt de aanvraag en 300 tot 600 cycli later is het resultaat beschikbaar. Dit betekent echter niet dat de geheugencontroller 300 tot 600 cycli stil staat. Er kan \'e\'en 128 bit aanvraag gedaan worden per geheugencyclus. Tijdens de 300 tot 600 cycli wordt de wavefront in een wachttoestand gezet en de compute-unit voert een andere wavefront uit. De wavefront wordt pas terug actief wanneer alle geheugenaanvragen van alle work-items volledig rond zijn.

Net als bij het lokaal geheugen bepaalt het adres naar welk kanaal de aanvraag gaat. De eerste 256 bytes gaan naar het eerste kanaal en naar de eerste bank van het kanaal, de volgende 256 gaan naar het tweede kanaal en diens eerste bank enzovoorts. De 2049ste byte gaat terug naar het eerste kanaal, maar dan in de tweede bank. Wanneer we door alle banken geroteerd zijn zal de volgende byte terug naar de eerste bank van het eerste kanaal gaan. Het aantal banken is echter niet gedocumenteerd.

Net als bij het lokaal geheugen kan het gebeuren dat meerdere work-item hetzelfde kanaal en/of dezelfde bank aanspreken. Wanneer dit gebeurt zullen de aanvragen geserialiseerd worden. Dit gebeurt zowel voor lees -als schrijfoperaties. We spreken zowel bij banken als bij kanalen over kanaalconflicten om het contrast met bankconflicten. We doen dit om het contrast met bankconflicten in het lokaal geheugen te bewaren.
\todo{Mooie tekeningen}



\subsubsection{Constant globaal geheugen, L1 -en L2 caches}
Omdat het globaal geheugen vrij traag is, zowel bandbreedte als traagheid, zijn er enkele caches ingebouwd. Elke compute-unit heeft een L1-cache en elk kanaal heeft een L2-cache. De programmeur heeft echter geen rechtstreekse controle over deze caches, maar men kan wel een betere performantie verkrijgen als men er wel rekening mee houdt. Caching wordt enkel toegepast in twee gevallen. Wanneer een aanvraag gaat naar geheugen dat nooit verandert om inconsistenties te vermijden en wanneer alle work-items van een wavefront in dezelfde cyclus hetzelfde adres uitlezen.

Wanneer het adres al bij het compilen bekend is en de inhoud niet van waarde verandert, dan wordt de inhoud opgeslagen in de constanten-cache. Deze inhoud wordt automatisch ingeladen vlak voor het gebruik ervan en levert snelheden tot 256 bytes per cyclus per compute-unit. Dit is twee maal zo snel als het lokaal geheugen en maar drie maal zo traag als de registers. Dit komt meestal voor bij eenvoudige constanten en kernel-parameter.

\todo{tabel snelheden}

\section{Aandachtspunten voor optimalisaties}

\subsection{Wavefronten}
Het is belangrijk om voldoende wavefronten actief te hebben om alle vormen van traagheid te maskeren. Vanaf dat er genoeg wavefronten actief zijn, heeft het geen zin meer om er meer actief te maken. Er zijn echter enkele limieten op het aantal wavefronten die gelijktijdig actief kunnen zijn. Indien er niet genoeg wavefronten zal zowel de compute-unit als de geheugencontroller stil vallen.

\subsubsection{Traagheid maskeren}
Wanneer we een instructie uitvoeren duurt het acht cycli voor het resultaat ervan beschikbaar is. En wavefront bestaat uit vier cycli, dus twee wavefronten per compute-unit volstaan om deze traagheid te maskeren.

Een globaal geheugenaanvraag duurt 300 tot 600 cycli. Laten we voor deze berekening 400 cycli nemen en stellen dat er na elke leesinstructie, vijf rekeninstructies zijn en dan terug een leesinstructie enzovoorts. Een wavefront bestaat uit vier cycli die elk vijf instructies uitvoeren voor ze zelf een geheugenaanvraag doen. Er zijn voor dit voorbeeld dus 20 (400 / (4 * 5)) wavefronten per compute-unit nodig om de traagheid van het globaal geheugen te maskeren.

\subsubsection{Limieten}
De eerste en meest eenvoudige limiet is dat er maximaal 512 wavefronten tegelijkertijd actief kunnen zijn. Dit zijn er 21 per compute-unit en dus net genoeg voor het bovenstaande voorbeeld.

De registers worden door alle work-items op een compute-unit gedeeld. Het aantal nodige registers per work-item is gekend na de compilatie. Stel dat elk workitem 32 registers nodig heeft. Dat zijn er 2Ki per wavefront. Er zijn 16 Ki registers beschikbaar, dus er kunnen dan exact acht wavefront gelijktijdig actief per compute-unit. \todo{verwijzen naar tabel pg 6-26}

Net als het aantal registers is ook de hoeveel lokaal geheugen gedeeld en beperkt. De hoeveelheid lokaal geheugen per work-group is gekend wanneer de kernel gelanceerd wordt. Stel dat een work-group uit 100 work-items bestaat en 4KiB lokaal geheugen nodig heeft. We hebben 32KiB lokaal geheugen, dus er kunnen acht work-groups gelijktijdig draaien op elke compute-unit. E\'en work-group komt overeen met twee wavefronten (64 < 100 < 128). Dit geeft dan 16 actieve wavefronten per compute-unit.
\todo{definieer een actief wavefront} \todo{verwijzen naar tabel pg 6-28}

\subsection{Globaal geheugen}
\subsubsection{Kanaalconflicten}

\subsubsection{FastPath of CompletePath}
Er zijn twee manieren waarop geheugen opgevraagd kan worden, via FastPath en via CompletePath. FastPath kan elk gegevens laden en opslaan. Deze gegevens moeten een veelvoud van 32 bits groot zijn. CompletePath kan ook gegevensstructuren verwerken die kleiner zijn dan 32 bit. Bijvoorbeeld bytes en short. CompletePath ondersteunt ook atomische operaties.

Deze extra functionaliteit komt echter met een grote kost. Empirische testen \todo{ref p6-3} hebben aangetoond dat CompletePath wel vier keer ze traag kan zijn. CompletePath moet dus zoveel mogelijk vermeden worden.

\subsubsection{Float4 of float}
Het globaal geheugen stuurt gegevens in blokken van 128 bytes. Dit komt overeen met een float4 of een double2. In een empirische test wordt een buffer gekopi\"eerd. Eerst met behulp van float's, daarna met behulp van float4's. Uit de test blijkt dat de bandbreedte bij het kopi\"eren met float4's ongeveer een derde hoger is dan met float's. \todo{ref p6-3}

\subsection{Lokaal geheugen}
\subsubsection{Float4}
Beschouw onderstaand fragment uit een kernel waarbij een work-group bestaat uit 64 work-items en de NDRange slechts \'e\'en dimensie heeft.
\begin{lstlisting}
	...
    __local float4* l[64];
    float4 v;
    ...
    v = l[get_local_id(0)];
    ...
\end{lstlisting}
Het lokaal geheugen kan twee vierbyte waarden verwerken, een float4 bestaat uit vier vierbyte waarden en een wavefront bestaat uit vier cycli. Men zou dus denken dat de geheugenaanvraag in totaal acht cycli in beslag zal nemen. Maar doordat eerst de eerste twee vierbytes opgeslagen worden en dan pas de volgende twee, worden elke cyclus slechts de helft van de banken gebruikt en zal het in totaal 16 cycli duren voor de geheugenaanvragen volledig rond zijn.\todo{tekening} Dit is een patroon dat vermeden moet worden.

\subsubsection{Alternatieven}
Er zijn alternatieven voor het lokaal geheugen die zeker overwogen moeten worden. Een eerste alternatief is de L1-cache. De L1-cache is niet rechtstreek controleerbaar en kan enkel gebruikt worden voor lees-operaties. Het is ook kleiner dan het lokaal geheugen. Maar het heeft ook voordelen. De L1-cache wordt gedeeld door alle actieve work-groups op de compute-unit en het gebeurt expliciet waardoor er geen extra cycli verloren gaan. Daarnaast krijgt het lokaal geheugen het moeilijk wanneer de gegevenstypes groter zijn dan acht bytes. (zie hierboven\ref{punt hierboven}) Het lokaal geheugen is maar twee maal zo snel als de L1-cache, maar moet wel eerst gevuld worden.

Een tweede alternatief zijn is het registergeheugen. Dit geheugen is niet gedeeld met de andere work-items waardoor er meerdere kopi\"en van de gegevens bestaan. Aan de andere kant is het registergeheugen acht keer zo groot en zes maal zo snel als het lokaal geheugen. Een ander nadeel van het registergeheugen is dat de indexen al tijdens het compilen vast moeten liggen. Net als het lokaal geheugen heeft de programmeur hier volledige controle over en moet het eerst ingeladen worden.

Het laatste alternatief is het contantengeheugen. Net als de L1-cache wordt dit geheugen impliciet gebruikt en is het enkel toegankelijk voor leesoperaties. De grootste nadelen zijn dat de adressen gekend moeten zijn bij compilatie en dat het om geheugen gaat dat enkel gelezen kan worden. Het is ook zeer beperkt in grootte. Het grootste voordelen zijn dat het twee maal zo snel is als het lokaal geheugen, geen problemen heeft met gegeventype die groter zijn dan acht bytes en dat het niet ingeladen moet worden.